{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da4f527-30ab-4568-9dec-b52ca56e9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st \n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings import OpenAIEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b466fde2-da17-4d51-82e9-67ab83198221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf_path):\n",
    "    text = \"\"\n",
    "    pdf_reader = PdfReader(pdf_path)\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "#지정된 조건에 따라 주어진 텍스트를 더 작은 덩어리로 분할\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=\"\\\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "#주어진 텍스트 청크에 대한 임베딩을 생성하고 FAISS를 사용하여 벡터 저장소를 생성\n",
    "def get_vectorstore(text_chunks):\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d9db575-df32-439d-84f9-1e40e3e23bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk\" #openai 키 입력\n",
    "\n",
    "#주어진 벡터 저장소로 대화 체인을 초기화\n",
    "def get_conversation_chain(vectorstore):\n",
    "    memory = ConversationBufferWindowMemory(memory_key='chat_history', return_message=True)  #ConversationBufferWindowMemory에 이전 대화 저장\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(temperature=0.7, model_name='gpt-3.5-turbo-16k-0613'),\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        get_chat_history=lambda h: h,\n",
    "        memory=memory\n",
    "    ) #ConversationalRetrievalChain을 통해 langchain 챗봇에 쿼리 전송\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e8de0f-5b9f-4f5c-b11d-7ed5f13b9561",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wnghk\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wnghk\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wnghk\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# PDF 파일 경로 설정\n",
    "pdf_path = \"The_Adventures_of_Tom_Sawyer.pdf\"\n",
    "# PDF 텍스트 가져오기\n",
    "raw_text = get_pdf_text(pdf_path)\n",
    "# 텍스트에서 청크 검색\n",
    "text_chunks = get_text_chunks(raw_text)\n",
    "# PDF 텍스트 저장을 위해 FAISS 벡터 저장소 만들기\n",
    "vectorstore = get_vectorstore(text_chunks)\n",
    "# 대화 체인 만들기\n",
    "m = get_conversation_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2f00412-1e5d-4b77-83ee-e6852b7db9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문을 해주세요.\n",
      "사용자: 그럼 언제 죽어\n",
      "주어진 텍스트에서는 주인공이 죽는 부분에 대한 언급이 없습니다. 따라서 주인공이 언제 죽는지에 대한 정보를 알 수 없습니다.\n",
      "질문을 해주세요.\n",
      "사용자: 여자 주인공 누구 있어\n",
      "여자 주인공은 Becky입니다.\n",
      "질문을 해주세요.\n",
      "사용자: 백희 말고는 없어\n",
      "주어진 정보에서는 백희 외에 다른 여자 주인공에 대한 언급이 없습니다.\n",
      "질문을 해주세요.\n",
      "사용자: 멈춰 멈춰 멈춰\n",
      "저는 그 질문에 대한 답을 모릅니다.\n",
      "질문을 해주세요.\n",
      "사용자: 멈춰\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "\n",
    "# 음성 입력 (STT)\n",
    "def recognize_speech():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"질문을 해주세요.\")\n",
    "        audio = recognizer.listen(source)\n",
    "    try:\n",
    "        question = recognizer.recognize_google(audio, language=\"ko-KR\")\n",
    "        print(\"사용자:\", question)\n",
    "        return question\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"음성을 이해할 수 없습니다.\")\n",
    "        return \"\"\n",
    "    except sr.RequestError:\n",
    "        print(\"음성 서비스에 접근할 수 없습니다.\")\n",
    "        return \"\"\n",
    "\n",
    "# 음성 출력 (TTS)\n",
    "def speak_response(response):\n",
    "    engine = pyttsx3.init()  # 기본 드라이버로 초기화\n",
    "    engine.say(response)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# 질문 처리 및 답변 생성\n",
    "def process_question(question):\n",
    "    # 질문에 대한 답변 가져오기\n",
    "    response = m({\"question\": question})\n",
    "    if 'answer' in response:\n",
    "        answer = response['answer']\n",
    "    else:\n",
    "        print(\"올바른 응답 키를 찾을 수 없습니다.\")\n",
    "    return answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        question = recognize_speech()\n",
    "        if question==\"멈춰\":\n",
    "            break;\n",
    "        if question:\n",
    "            answer = process_question(question)\n",
    "            print(answer)\n",
    "            speak_response(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9519861f-baa0-4e5c-97b1-2ec414c175de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
